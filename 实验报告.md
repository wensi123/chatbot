===== User =====
你好，我需要你的帮助来开发一个聊天机器人Web应用。
目标： 创建一个功能完整的聊天机器人，用户可以通过网页界面与之交互。
开发环境： 我正在使用 windows系统下的VS Code，并已经安装了Python 相关扩展。
我希望在项目目录中使用 conda虚拟环境。
请指导我完成项目的初始设置步骤。
===== AI =====
 好的，我们来一步步完成聊天机器人 Web 应用的初始设置。

**目标：** 准备好开发环境和项目基础结构。

**步骤 1：创建项目目录结构**

首先，在你的 Windows 系统上找一个合适的位置（比如 `C:\Users\YourUser\Documents\Projects\`），然后创建一个新的项目文件夹。我们称之为 `chatbot-app`。

打开 VS Code，然后通过 `文件 (File)` > `打开文件夹 (Open Folder)...` 选择你刚刚创建的 `chatbot-app` 文件夹。

在 VS Code 的终端（你可以通过 `终端 (Terminal)` > `新建终端 (New Terminal)` 或快捷键 `Ctrl+`` 打开）中，我们将创建以下目录结构：

```bash
# (确保你当前在 VS Code 终端中，并且路径是 chatbot-app)
mkdir templates
mkdir static
touch app.py
cd templates
touch index.html
cd ..
cd static
touch script.js
# 你也可以选择添加一个简单的 CSS 文件 (可选)
# touch style.css
cd ..
```

现在你的项目结构应该看起来像这样：

```
chatbot-app/
├── app.py           # Flask 后端应用
├── static/          # 存放前端静态文件 (JS, CSS)
│   └── script.js    # 前端 JavaScript 代码
│   #└── style.css   # (可选) 前端 CSS 样式
└── templates/       # 存放 Flask 渲染的 HTML 模板
    └── index.html   # 前端主页面
```

**步骤 2：创建并激活 Conda 虚拟环境**

在 VS Code 的终端中（确保当前目录是 `chatbot-app`），执行以下命令来创建 Conda 环境。我们将环境命名为 `chatbot_env`，并指定 Python 版本（推荐 3.9 或 3.10，对 Hugging Face 库兼容性较好）。

```bash
# 创建环境 (如果conda命令不可用，请确保Conda已添加到系统PATH或使用Anaconda Prompt)
conda create -n chatbot_env python=3.9 -y

# 激活环境
conda activate chatbot_env
```

激活环境后，你的终端提示符前面应该会显示 `(chatbot_env)`。**请确保之后的所有命令都在这个激活的环境中执行。**

**步骤 3：在 VS Code 中选择 Python 解释器**

1.  按 `Ctrl+Shift+P` 打开命令面板。
2.  输入 `Python: Select Interpreter` (或中文 `Python: 选择解释器`)。
3.  从列表中选择带有 `chatbot_env` 字样的 Python 解释器。这能确保 VS Code 使用你虚拟环境中的 Python 和库进行代码分析、调试等。

===== User =====
好的 现在我需要继续完善这个项目
后端需要能够加载 Hugging Face transformers 库中的预训练文本生成语言模型 模型的生成使用cpu（模型已经下载至.\Qwen1.5-1.8B-Chat_model\models--Qwen--Qwen1.5-1.8B-Chat\snapshots\e482ee3f73c375a627a16fdf66fd0c8279743ca6）。
前端接收的用户输入，调用模型生成回复，并将回复返回给前端。前端界面类似于chatgpt，使用简单的技术即可（html、js、css）。
需要实现流式输出，让模型的回复能够像打字一样逐步显示在前端，而不是等待完全生成后再显示。
过程中可以加入简单的单元测试。
===== AI =====
 好的，我们可以分步骤来完成这个任务。

**项目结构：**

```
chatbot_app/
├── app.py                     # Flask 后端应用
├── static/
│   ├── script.js
│   └── style.css
├── templates/
│   └── index.html
├── tests/
│   └── test_app.py
├── Qwen1.5-1.8B-Chat_model/   # 你提供的模型文件夹
│   └── models--Qwen--Qwen1.5-1.8B-Chat/
│       └── snapshots/
│           └── e482ee3f73c375a627a16fdf66fd0c8279743ca6/
│               └── # (模型文件在这里)
└── requirements.txt           # 项目依赖
```

**步骤 1: 安装必要的库**

首先，创建一个 `requirements.txt` 文件：

```txt
Flask
transformers
torch
sentencepiece # Qwen tokenizer 可能需要
accelerate # 推荐，即使是CPU，有时也能优化加载
```

然后安装它们：
`pip install -r requirements.txt`

**步骤 2: 后端 (Flask - `app.py`)**

我们将使用 Flask 来创建后端 API。模型加载会比较耗时，所以我们会在应用启动时加载一次。流式输出将通过 Server-Sent Events (SSE) 实现，这需要模型生成在后台线程中运行。

```python
# chatbot_app/app.py
import os
import threading
from flask import Flask, render_template, request, Response, stream_with_context, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, GenerationConfig
import torch

# --- 配置 ---
MODEL_PATH = os.path.join(
    ".", "Qwen1.5-1.8B-Chat_model", "models--Qwen--Qwen1.5-1.8B-Chat", 
    "snapshots", "e482ee3f73c375a627a16fdf66fd0c8279743ca6"
)
# 确保模型路径正确，如果你的脚本在 chatbot_app 目录下运行，上面的路径是正确的
# 如果你在 chatbot_app 的父目录运行 python chatbot_app/app.py, 则路径应相应调整或使用绝对路径

# --- 全局变量 ---
model = None
tokenizer = None
device = "cpu" # 明确使用CPU

# --- 应用创建函数 ---
def create_app():
    app = Flask(__name__)

    # --- 模型加载 ---
    def load_model():
        global model, tokenizer
        print(f"Loading model from: {MODEL_PATH}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                torch_dtype="auto", #可以尝试 torch.float32 如果 "auto" 有问题
                device_map=device, # 指定使用CPU
                trust_remote_code=True
            )
            model.eval() # 设置为评估模式
            print("Model loaded successfully on CPU.")
        except Exception as e:
            print(f"Error loading model: {e}")
            # 实际应用中可能需要更健壮的错误处理
            raise

    load_model() # 应用启动时加载模型

    # --- 路由 ---
    @app.route('/')
    def index():
        return render_template('index.html')

    @app.route('/chat', methods=['POST'])
    def chat():
        try:
            data = request.json
            user_input = data.get('message')
            history = data.get('history', []) # 接收聊天历史

            if not user_input:
                return jsonify({"error": "No message provided"}), 400

            messages = []
            for entry in history:
                messages.append({"role": entry["role"], "content": entry["content"]})
            messages.append({"role": "user", "content": user_input})
            
            # 使用 apply_chat_template 构建输入
            # Qwen1.5-Chat 的模板会自动处理多轮对话格式
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True # 重要：为模型生成添加起始提示
            )
            
            model_inputs = tokenizer([text], return_tensors="pt").to(device)

            streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
            
            generation_config = GenerationConfig(
                max_new_tokens=1024,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.05,
                # streamer=streamer, # streamer参数在generate调用时传入
                # eos_token_id=tokenizer.eos_token_id # Qwen1.5可能不需要显式设置这个
            )
            
            # generation_kwargs 用于传递给 generate 方法
            generation_kwargs = {
                "input_ids": model_inputs.input_ids,
                "attention_mask": model_inputs.attention_mask,
                "generation_config": generation_config,
                "streamer": streamer,
            }

            # 在一个单独的线程中运行生成，以允许流式处理
            thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()

            def generate_responses():
                for new_text in streamer:
                    yield f"data: {new_text}\n\n" # SSE格式
                # 可选：发送一个特殊的结束标记
                # yield f"data: [END_OF_STREAM]\n\n"

            return Response(stream_with_context(generate_responses()), mimetype='text/event-stream')

        except Exception as e:
            print(f"Error during chat generation: {e}")
            # 在流式传输中，错误处理比较棘手，这里简单打印
            # 可以考虑发送一个错误事件给客户端
            def error_stream():
                yield f"data: [ERROR] An error occurred: {str(e)}\n\n"
            return Response(stream_with_context(error_stream()), mimetype='text/event-stream', status=500)

    return app

# --- 主程序入口 ---
if __name__ == '__main__':
    app = create_app()
    # 修改端口避免与常见端口冲突，debug=True 用于开发
    app.run(host='0.0.0.0', port=5001, debug=True)
```

**关键点解释 (后端):**

1.  **模型加载**: `load_model()` 在应用启动时加载模型和分词器到CPU。
2.  **`/chat` 路由**:
    *   接收 POST 请求，包含用户消息 `message` 和可选的聊天历史 `history`。
    *   `tokenizer.apply_chat_template`: 这是处理聊天模型（如Qwen）输入格式的关键。它会根据提供的消息列表（角色和内容）构建正确的提示。`add_generation_prompt=True` 确保模型知道它应该开始生成回复。
    *   `TextIteratorStreamer`: 这个类是 `transformers` 库提供的，用于在文本生成过程中逐个token或小块文本地迭代。
    *   `threading.Thread`: `model.generate()` 是一个阻塞操作。为了实现流式输出，我们必须在单独的线程中运行它。主线程则可以从 `streamer` 中读取并发送数据。
    *   `generate_responses()`: 这是一个生成器函数，它迭代 `streamer` 产生的内容，并将其格式化为 Server-Sent Events (SSE) 的格式 (`data: ...\n\n`)。
    *   `Response(stream_with_context(...), mimetype='text/event-stream')`: Flask 的方式来发送流式响应。

**步骤 3: 前端 (HTML, CSS, JS)**

**`templates/index.html`:**

```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>聊天机器人</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="chat-container">
        <div id="chat-box" class="chat-box">
            <!-- 聊天消息将在这里显示 -->
        </div>
        <div class="input-area">
            <textarea id="user-input" placeholder="输入你的消息..." rows="2"></textarea>
            <button id="send-button">发送</button>
        </div>
    </div>
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>
```

**`static/style.css`:**

```css
body {
    font-family: Arial, sans-serif;
    margin: 0;
    background-color: #f4f4f9;
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    color: #333;
}

.chat-container {
    width: 90%;
    max-width: 600px;
    height: 80vh;
    max-height: 700px;
    background-color: #fff;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    display: flex;
    flex-direction: column;
    overflow: hidden;
}

.chat-box {
    flex-grow: 1;
    padding: 20px;
    overflow-y: auto;
    border-bottom: 1px solid #eee;
}

.message {
    margin-bottom: 15px;
    padding: 10px 15px;
    border-radius: 18px;
    line-height: 1.4;
    max-width: 80%;
    word-wrap: break-word;
}

.user-message {
    background-color: #007bff;
    color: white;
    margin-left: auto;
    border-bottom-right-radius: 5px;
}

.bot-message {
    background-color: #e9ecef;
    color: #333;
    margin-right: auto;
    border-bottom-left-radius: 5px;
}

.input-area {
    display: flex;
    padding: 15px;
    background-color: #f8f9fa;
}

#user-input {
    flex-grow: 1;
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 20px;
    margin-right: 10px;
    resize: none;
    font-size: 1rem;
    min-height: 24px; /* Min height for one line */
    max-height: 100px; /* Max height for ~4-5 lines */
    overflow-y: auto; /* Add scroll if text exceeds max-height */
}

#send-button {
    padding: 10px 20px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 20px;
    cursor: pointer;
    font-size: 1rem;
}

#send-button:hover {
    background-color: #0056b3;
}

/* Typing indicator (optional, simple version) */
.typing-indicator span {
    display: inline-block;
    width: 8px;
    height: 8px;
    margin: 0 2px;
    background-color: #aaa;
    border-radius: 50%;
    animation: blink 1.4s infinite both;
}

.typing-indicator span:nth-child(2) {
    animation-delay: 0.2s;
}

.typing-indicator span:nth-child(3) {
    animation-delay: 0.4s;
}

@keyframes blink {
    0% { opacity: 0.2; }
    20% { opacity: 1; }
    100% { opacity: 0.2; }
}
```

**`static/script.js`:**

```javascript
document.addEventListener('DOMContentLoaded', () => {
    const chatBox = document.getElementById('chat-box');
    const userInput = document.getElementById('user-input');
    const sendButton = document.getElementById('send-button');
    let chatHistory = []; // 用于存储对话历史

    // 调整textarea高度
    userInput.addEventListener('input', () => {
        userInput.style.height = 'auto'; // Reset height
        userInput.style.height = userInput.scrollHeight + 'px'; // Set to scroll height
    });

    sendButton.addEventListener('click', sendMessage);
    userInput.addEventListener('keypress', (event) => {
        if (event.key === 'Enter' && !event.shiftKey) {
            event.preventDefault();
            sendMessage();
        }
    });

    function appendMessage(text, sender) {
        const messageDiv = document.createElement('div');
        messageDiv.classList.add('message', sender === 'user' ? 'user-message' : 'bot-message');
        
        // 为了安全，对文本进行转义 (简单实现，实际应用可能需要更完善的库)
        const textNode = document.createTextNode(text);
        messageDiv.appendChild(textNode);
        
        chatBox.appendChild(messageDiv);
        chatBox.scrollTop = chatBox.scrollHeight; // 滚动到底部
        return messageDiv; // 返回消息元素，用于流式更新
    }

    async function sendMessage() {
        const messageText = userInput.value.trim();
        if (messageText === '') return;

        appendMessage(messageText, 'user');
        chatHistory.push({ role: "user", content: messageText });
        userInput.value = '';
        userInput.style.height = 'auto'; // Reset height after sending
        sendButton.disabled = true; // 禁用发送按钮，直到收到回复

        // 为机器人回复创建一个占位符
        let botMessageDiv = appendMessage('', 'bot');
        // 可以添加一个打字指示器
        botMessageDiv.innerHTML = '<div class="typing-indicator"><span></span><span></span><span></span></div>';


        try {
            const response = await fetch('/chat', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    message: messageText,
                    history: chatHistory // 发送包含当前消息的完整历史
                }),
            });

            if (!response.ok) {
                // 如果HTTP状态码不是2xx，尝试读取错误信息
                let errorText = `服务器错误: ${response.status}`;
                try {
                    const errorData = await response.json(); // 假设服务器返回JSON错误
                    errorText = errorData.error || errorText;
                } catch (e) {
                    // 如果响应不是JSON或解析失败
                    errorText = await response.text() || errorText;
                }
                console.error('Error from server:', errorText);
                botMessageDiv.textContent = `错误: ${errorText}`;
                chatHistory.pop(); // 如果发送失败，从历史中移除用户最后一条消息
                sendButton.disabled = false;
                return;
            }
            
            // 使用 ReadableStream 处理流式响应
            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let fullBotResponse = "";
            botMessageDiv.innerHTML = ""; // 清除打字指示器

            while (true) {
                const { value, done } = await reader.read();
                if (done) {
                    break;
                }
                const chunk = decoder.decode(value, { stream: true });
                // SSE 数据格式是 "data: content\n\n"
                // 我们需要解析出 content
                const lines = chunk.split('\n');
                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.substring(6).trim();
                        if (data === "[END_OF_STREAM]") { // 可选的结束标记
                            // stream ended
                        } else if (data.startsWith("[ERROR]")) {
                            botMessageDiv.textContent += data.substring(7).trim();
                            fullBotResponse += data.substring(7).trim();
                        } else {
                            botMessageDiv.textContent += data; // 逐字追加
                            fullBotResponse += data;
                            chatBox.scrollTop = chatBox.scrollHeight;
                        }
                    }
                }
            }
            // 流结束后，将完整的机器人回复添加到历史记录
            if (fullBotResponse) {
                chatHistory.push({ role: "assistant", content: fullBotResponse });
            } else { // 如果机器人没有回复任何内容（例如，只有错误或空流）
                chatBox.removeChild(botMessageDiv); // 移除空的机器人消息框
                // chatHistory.pop(); // 用户消息已添加，不需要移除
            }

        } catch (error) {
            console.error('Error sending message:', error);
            botMessageDiv.textContent = '发生网络错误，请稍后再试。';
            // chatHistory.pop(); // 移除用户消息，因为没有成功交互
        } finally {
            sendButton.disabled = false; // 重新启用发送按钮
        }
    }
});
```

**关键点解释 (前端):**

1.  **HTML 结构**: 简单的聊天框和输入区域。
2.  **CSS 样式**: 基础的聊天气泡样式。
3.  **JavaScript (`script.js`)**:
    *   **`sendMessage()`**:
        *   获取用户输入，显示用户消息。
        *   将用户消息添加到 `chatHistory`。
        *   **`fetch('/chat', ...)`**: 发送 POST 请求到后端。
        *   **`response.body.getReader()`**: 获取响应体的 `ReadableStream` 读取器。这是现代浏览器处理流式响应的标准方式，比 `EventSource` 更灵活，因为 `EventSource` 仅支持 GET 请求，且对错误处理和自定义头部支持有限。
        *   **`reader.read()`**: 循环读取数据块。
        *   **`decoder.decode(value, { stream: true })`**: 将 `Uint8Array` 数据块解码为字符串。
        *   **解析 SSE 数据**: 从 `data: ...` 行中提取实际的文本内容。
        *   **逐字显示**: `botMessageDiv.textContent += data;` 将每个小块文本追加到机器人的消息气泡中，实现打字效果。
        *   **滚动到底部**: `chatBox.scrollTop = chatBox.scrollHeight;` 确保最新的消息可见。
        *   **历史记录**: 完整的机器人回复收集后，添加到 `chatHistory`。
    *   **`textarea`自动调整高度**: 使得输入框可以根据内容多少自动伸缩。
    *   **错误处理**: 增加了对网络错误和服务器端错误的简单处理。
    *   **发送按钮禁用**: 在请求过程中禁用发送按钮，防止重复发送。

**步骤 4: 单元测试 (后端 - `tests/test_app.py`)**

这是一个简单的单元测试示例，使用 `unittest` 和 Flask 的测试客户端。

```python
# chatbot_app/tests/test_app.py
import unittest
import sys
import os
from unittest.mock import patch, MagicMock

# 将 chatbot_app 的父目录添加到 sys.path，以便可以导入 app
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# 在导入 app 模块之前设置环境变量或进行其他配置（如果需要）
# 例如，如果模型加载很慢，可以考虑在测试中 mock 掉它
# os.environ['FLASK_ENV'] = 'testing' # 如果你的 app 根据这个变量有不同行为

from app import create_app # 导入应用工厂

class ChatbotAppTestCase(unittest.TestCase):

    def setUp(self):
        # Mock 模型和分词器加载，避免实际加载耗时模型
        self.mock_tokenizer = MagicMock()
        self.mock_tokenizer.apply_chat_template.return_value = "Formatted prompt"
        self.mock_tokenizer.return_value.to.return_value = MagicMock(input_ids="mock_input_ids", attention_mask="mock_attention_mask")

        self.mock_model = MagicMock()
        # mock_streamer = MagicMock(spec=TextIteratorStreamer) # 更精确的mock
        
        # 模拟 streamer 的迭代行为
        class MockStreamer:
            def __init__(self, tokenizer, skip_prompt, skip_special_tokens):
                pass
            def __iter__(self):
                yield "Hello"
                yield " "
                yield "World"
                yield "!"
        
        # 当 TextIteratorStreamer 被实例化时，返回我们的 MockStreamer 实例
        self.mock_text_iterator_streamer_class = MagicMock(return_value=MockStreamer(None, True, True))
        
        # 使用 patch 来替换 app.py 中的全局 model 和 tokenizer
        # 以及 TextIteratorStreamer 类
        self.patches = [
            patch('app.model', self.mock_model),
            patch('app.tokenizer', self.mock_tokenizer),
            patch('app.TextIteratorStreamer', self.mock_text_iterator_streamer_class)
        ]
        for p in self.patches:
            p.start()
            self.addCleanup(p.stop) # 确保 patch 在测试结束时停止

        self.app = create_app()
        self.app.config['TESTING'] = True
        self.client = self.app.test_client()

    def tearDown(self):
        # 清理工作，虽然 addCleanup 已经处理了 patch.stop
        pass

    def test_index_page(self):
        response = self.client.get('/')
        self.assertEqual(response.status_code, 200)
        self.assertIn(b'聊天机器人', response.data)

    def test_chat_stream(self):
        # 模拟 model.generate 的行为，因为它在线程中运行
        # 我们主要测试 streamer 是否被正确调用和数据是否被正确 yield
        # model.generate 本身不直接返回值给主线程，而是通过 streamer
        
        # 这里我们不需要模拟 model.generate 的内部，因为 streamer 的行为已经被 mock 了
        
        response = self.client.post('/chat', json={'message': '你好', 'history': []})
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.mimetype, 'text/event-stream')

        # 收集流式响应数据
        stream_data = b""
        for chunk in response.response: # response.response 是一个迭代器
            stream_data += chunk
        
        decoded_data = stream_data.decode('utf-8')
        
        # 验证 TextIteratorStreamer 被正确实例化
        self.mock_text_iterator_streamer_class.assert_called_once()
        
        # 验证 model.generate 被调用 (虽然它在线程中，但我们可以检查它是否被触发)
        # 注意：由于 generate 在线程中，直接检查其调用次数可能复杂。
        # 我们更关注 streamer 是否按预期工作，这间接证明了 generate 被调用。
        self.mock_model.generate.assert_called_once()
        
        # 检查 apply_chat_template 是否被调用
        self.mock_tokenizer.apply_chat_template.assert_called_once()
        args, kwargs = self.mock_tokenizer.apply_chat_template.call_args
        self.assertEqual(args[0], [{"role": "user", "content": "你好"}]) # 检查传入的消息
        self.assertTrue(kwargs.get('add_generation_prompt'))


        # 检查流式输出的内容是否符合预期（基于 MockStreamer 的输出）
        expected_sse_data = "data: Hello\n\ndata:  \n\ndata: World\n\ndata: !\n\n"
        self.assertEqual(decoded_data, expected_sse_data)

    def test_chat_no_message(self):
        response = self.client.post('/chat', json={})
        self.assertEqual(response.status_code, 400)
        json_data = response.get_json()
        self.assertEqual(json_data['error'], 'No message provided')

    def test_chat_with_history(self):
        history = [{"role": "user", "content": "之前的问题"}, {"role": "assistant", "content": "之前的回答"}]
        response = self.client.post('/chat', json={'message': '新问题', 'history': history})
        self.assertEqual(response.status_code, 200)
        
        # 检查 apply_chat_template 是否正确处理了历史记录
        self.mock_tokenizer.apply_chat_template.assert_called_once()
        args, kwargs = self.mock_tokenizer.apply_chat_template.call_args
        expected_messages = [
            {"role": "user", "content": "之前的问题"},
            {"role": "assistant", "content": "之前的回答"},
            {"role": "user", "content": "新问题"}
        ]
        self.assertEqual(args[0], expected_messages)


if __name__ == '__main__':
    unittest.main()
```

**运行测试:**

在 `chatbot_app` 目录下（或者说 `tests` 目录的父目录）运行:
`python -m unittest tests.test_app`
或者直接运行 `python tests/test_app.py` （如果 `sys.path` 设置正确）。

**关键点解释 (测试):**

1.  **`unittest.mock.patch`**: 用于替换 `app.py` 中的全局 `model`, `tokenizer` 和 `TextIteratorStreamer` 类，这样测试就不会真正加载大模型或执行耗时的生成操作。
2.  **`MockStreamer`**: 一个自定义的类，模拟 `TextIteratorStreamer` 的迭代行为，返回预设的文本块。
3.  **`self.client.post('/chat', ...)`**: Flask 测试客户端发送请求。
4.  **检查响应**: 验证状态码、MIME类型以及流式输出的内容。
5.  **`response.response`**: 在 Flask 测试客户端中，这是一个迭代器，可以用来获取流式响应的各个部分。
6.  **`assert_called_once_with` / `assert_called_once`**: 验证 mock 对象的方法是否被正确调用。

**步骤 5: 运行应用**

1.  确保你的模型文件在 `chatbot_app/Qwen1.5-1.8B-Chat_model/...` 路径下。
2.  在 `chatbot_app` 目录下运行后端:
    `python app.py`
3.  应用启动后，你应该会看到类似 "Loading model from: ..." 和 "Model loaded successfully on CPU." 的日志。
4.  打开浏览器，访问 `http://127.0.0.1:5001` (或 `http://localhost:5001`)。

现在你应该拥有一个可以工作的、具有流式输出的聊天机器人Web应用了！

**重要注意事项和潜在改进:**

*   **模型路径**: 仔细检查 `MODEL_PATH` 是否正确指向你下载的模型快照文件夹。
*   **错误处理**: 当前的错误处理比较基础。生产环境中需要更健壮的错误报告和恢复机制。
*   **安全性**: 前端显示的文本应该进行适当的HTML转义，防止XSS攻击。`createTextNode` 提供了一定的保护，但如果你用 `innerHTML` 插入复杂内容，需要小心。
*   **并发和性能**: Flask 的开发服务器是单线程的。对于生产环境，你应该使用像 Gunicorn 或 uWSGI 这样的 WSGI 服务器，并配置多个工作进程/线程。由于模型推理是 CPU 密集型操作且在单独线程中，这可以处理一定并发，但大量并发请求仍可能导致瓶颈。
*   **上下文长度**: Qwen模型有其上下文长度限制。如果聊天历史过长，需要实现截断或摘要策略。
*   **UI/UX**: 当前UI非常基础，可以进一步美化，增加加载指示、Markdown支持等。
*   **`TextIteratorStreamer` vs `EventSource`**: 我在前端改用了 `fetch` API 配合 `ReadableStream`，因为它对于POST请求和更复杂的流控制比 `EventSource` (通常用于GET的SSE) 更为直接和现代。后端仍然以SSE格式发送数据，前端解析这种格式。
*   **依赖项**: `sentencepiece` 通常是很多 Hugging Face tokenizer 的依赖。`accelerate` 有时可以帮助优化模型加载和推理，即使在CPU上。
*   **Qwen Chat Template**: Qwen1.5 Chat模型对于聊天格式有特定的要求，`tokenizer.apply_chat_template` 是正确处理这一点的关键。确保 `add_generation_prompt=True`，这样模型知道它应该接着生成回复。

这个方案应该能满足你的需求。测试并根据实际情况调整。
===== User =====
将上述代码进行单元测试时发生以下错误 应该如何修改单元测试代码
(chatbot_env) PS D:\file\桌面\软件测试技术\chatai> python -m unittest tests.test_app 
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\__main__.py", line 18, in <module>
    main(module=None)
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\main.py", line 104, in __init__
    self.parseArgs(argv)
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\main.py", line 153, in parseArgs
    self.createTests()
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\main.py", line 164, in createTests
    self.test = self.testLoader.loadTestsFromNames(self.testNames,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\loader.py", line 207, in loadTestsFrmes]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\loader.py", line 137, in loadTestsFromName
    module = __import__(module_name)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 62
    self.assertIn(b'聊天机器人', response.data)
                  ^^^^^^^^
SyntaxError: bytes can only contain ASCII literal characters
(chatbot_env) PS D:\file\桌面\软件测试技术\chatai> python -m unittest tests.test_app
Loading model from: .\Qwen1.5-1.8B-Chat_model\models--Qwen--Qwen1.5-1.8B-Chat\snapshots\e482ee3f73c375a627a16fdf66fd0c8279743ca6
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Model loaded successfully on CPU.
.Loading model from: .\Qwen1.5-1.8B-Chat_model\models--Qwen--Qwen1.5-1.8B-Chat\snapshots\e482ee3f73c375a627a16fdf66fd0c8279743ca6
Model loaded successfully on CPU.
FLoading model from: .\Qwen1.5-1.8B-Chat_model\models--Qwen--Qwen1.5-1.8B-Chat\snapshots\e482ee3f73c375a627a16fdf66fd0c8279743ca6
Exception in thread Thread-1 (generate):
Traceback (most recent call last):
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\site-packages\transformers\generation\utils.py", line 2300, in generate
    streamer.put(input_ids.cpu())
    ^^^^^^^^^^^^
AttributeError: 'MockStreamer' object has no attribute 'put'
Model loaded successfully on CPU.
Exception in thread Thread-2 (generate):
FLoading model from: .\Qwen1.5-1.8B-Chat_model\models--Qwen--Qwen1.5-1.8B-Chat\snapshots\e482ee3f73c375a627a16fdf66fd0c8279743ca6
Traceback (most recent call last):
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\site-packages\transformers\generation\utils.py", line 2300, in generate
    streamer.put(input_ids.cpu())
    ^^^^^^^^^^^^
AttributeError: 'MockStreamer' object has no attribute 'put'
Model loaded successfully on CPU.
F
======================================================================
FAIL: test_chat_stream (tests.test_app.ChatbotAppTestCase.test_chat_stream)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 88, in test_chat_stream      
    self.mock_model.generate.assert_called_once()
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 928, in assert_called_once
    raise AssertionError(msg)
AssertionError: Expected 'generate' to have been called once. Called 0 times.

======================================================================
FAIL: test_chat_with_history (tests.test_app.ChatbotAppTestCase.test_chat_with_history)        
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 113, in test_chat_with_history
    self.mock_tokenizer.apply_chat_template.assert_called_once()
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 928, in assert_called_once
    raise AssertionError(msg)
AssertionError: Expected 'apply_chat_template' to have been called once. Called 0 times.       

======================================================================
FAIL: test_index_page (tests.test_app.ChatbotAppTestCase.test_index_page)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 62, in test_index_page       
    self.assertIn(b'chatai', response.data)
AssertionError: b'chatai' not found in b'<!DOCTYPE html>\n<html lang="zh-CN">\n<head>\n    <meta charset="UTF-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1.0">\n    <title>\xe8\x81\x8a\xe5\xa4\xa9\xe6\x9c\xba\xe5\x99\xa8\xe4\xba\xba</title>\n    <link rel="stylesheet" href="/static/style.css">\n</head>\n<body>\n    <div class="chat-container">\n        <div id="chat-box" class="chat-box">\n            <!-- \xe8\x81\x8a\xe5\xa4\xa9\xe6\xb6\x88\xe6\x81\xaf\xe5\xb0\x86\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xbe\xe7\xa4\xba -->\n        </div>\n        <div class="input-area">\n            <textarea id="user-input" placeholder="\xe8\xbe\x93\xe5\x85\xa5\xe4\xbd\xa0\xe7\x9a\x84\xe6\xb6\x88\xe6\x81\xaf..." rows="2"></textarea>\n            <button id="send-button">\xe5\x8f\x91\xe9\x80\x81</button>\n        </div>\n    </div>\n    <script src="/static/script.js"></script>\n</body>\n</html>'

----------------------------------------------------------------------
Ran 4 tests in 4.128s

FAILED (failures=3)
===== AI =====
 好的，看来测试遇到了一些问题。我们来逐个分析并修复。

**问题1: `SyntaxError: bytes can only contain ASCII literal characters`**

在 `tests/test_app.py` 的 `test_index_page` 方法中：
```python
self.assertIn(b'聊天机器人', response.data)
```
Python 的 `bytes` 字面量 `b''` 只能包含 ASCII 字符。"聊天机器人" 是中文字符。

**修复1:**
将 `response.data` (字节串) 解码为字符串，然后与普通字符串比较。或者将要查找的字符串编码为字节串。推荐前者，更易读。

修改 `test_index_page`：
```python
    def test_index_page(self):
        response = self.client.get('/')
        self.assertEqual(response.status_code, 200)
        # 解码 response.data 为 utf-8 字符串
        self.assertIn('聊天机器人', response.data.decode('utf-8'))
```

---

**问题2: `AttributeError: 'MockStreamer' object has no attribute 'put'`**

当 `transformers` 库中的 `model.generate` 方法使用 `streamer` 时，它会调用 `streamer.put()` 和 `streamer.end()` 方法。我们自定义的 `MockStreamer` 需要实现这两个方法。

**修复2:**
修改 `MockStreamer` 类：
```python
class MockStreamer:
    def __init__(self, tokenizer, skip_prompt, skip_special_tokens):
        # 这些参数在 mock 中可能不需要，但保持签名一致
        self.tokenizer = tokenizer
        self.skip_prompt = skip_prompt
        self.skip_special_tokens = skip_special_tokens
        # self.queue = [] # 如果需要模拟更复杂的行为

    def put(self, value):
        """
        Mock for the put method. In a real TextIteratorStreamer,
        this method is called by the generation function to add new tokens.
        """
        # For our simple mock, we can just pass or log the call.
        # The actual iteration logic is in __iter__.
        pass

    def end(self):
        """
        Mock for the end method. In a real TextIteratorStreamer,
        this method is called by the generation function to signal the end of generation.
        """
        pass

    def __iter__(self):
        yield "Hello"
        yield " "
        yield "World"
        yield "!"

    def __next__(self): # 如果 TextIteratorStreamer 需要是迭代器本身
        # 这个简单的 __iter__ 已经让它是可迭代的了
        # 如果需要更复杂的 next 行为，可以实现
        if not hasattr(self, '_iter'):
            self._iter = self.__iter__()
        try:
            return next(self._iter)
        except StopIteration:
            del self._iter # Reset for next potential iteration if any
            raise
```

---

**问题3: `AssertionError: Expected 'generate' to have been called once. Called 0 times.` 和 `AssertionError: Expected 'apply_chat_template' to have been called once. Called 0 times.`**

以及日志中反复出现的 "Loading model from: ..." 和 "Model loaded successfully on CPU."，甚至还有 "Sliding Window Attention..." 警告。
这表明我们的 patching 可能没有完全阻止真实模型的加载，或者 mock 对象没有被正确使用。

**核心原因分析：**
`app.py` 中 `load_model()` 是在模块级别定义的，并且在 `create_app()` 函数内部被调用。
`model` 和 `tokenizer` 是全局变量。
当我们在 `setUp` 中执行 `self.app = create_app()` 时，`load_model()` 会被调用。
如果 patch 的是 `app.model` 和 `app.tokenizer` (即模块的全局变量)，这应该在 `load_model` *之后* 替换它们。
但是，如果 `load_model` 内部直接调用 `AutoModelForCausalLM.from_pretrained` 并将其结果赋值给全局变量，那么在路由函数中使用这些全局变量时，它们已经是真实模型了（如果 patch 晚于加载）。

**更稳健的 Patching 策略：Patch `from_pretrained` 调用本身。**

**修复3:**
修改 `setUp` 方法，不再 patch 全局变量 `app.model` 和 `app.tokenizer`，而是 patch `transformers.AutoModelForCausalLM.from_pretrained` 和 `transformers.AutoTokenizer.from_pretrained`，以及 `app.TextIteratorStreamer`（如果 `app.py` 中是以 `from transformers import TextIteratorStreamer` 然后直接使用 `TextIteratorStreamer(...)` 的话，patch `app.TextIteratorStreamer` 或 `transformers.TextIteratorStreamer` 都可以，前者更精确地表示 `app.py` 中使用的那个名字）。

新的 `setUp`：
```python
# tests/test_app.py
import unittest
import sys
import os
from unittest.mock import patch, MagicMock

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app import create_app # 导入应用工厂
# 明确地从 transformers 导入被 mock 的类，以便 spec 参数能正确工作
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer as TransformersTextIteratorStreamer

# ... (MockStreamer class as defined in Fix 2) ...

class ChatbotAppTestCase(unittest.TestCase):

    def setUp(self):
        # 1. Mock for AutoTokenizer.from_pretrained
        self.mock_tokenizer_instance = MagicMock(spec=AutoTokenizer) # 使用 spec
        self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"
        
        # Mocking tokenizer([text], return_tensors="pt").to(device)
        # tokenizer() call returns an object (e.g., BatchEncoding)
        mock_tokenized_output = MagicMock()
        # This object has a .to() method
        mock_inputs_on_device = MagicMock(input_ids="mock_input_ids", attention_mask="mock_attention_mask")
        mock_tokenized_output.to.return_value = mock_inputs_on_device
        self.mock_tokenizer_instance.return_value = mock_tokenized_output # So tokenizer(...) returns mock_tokenized_output

        # 2. Mock for AutoModelForCausalLM.from_pretrained
        self.mock_model_instance = MagicMock(spec=AutoModelForCausalLM) # 使用 spec
        # model.generate will be called in a thread. Its streamer arg will be our mock_streamer_instance.

        # 3. Mock for TextIteratorStreamer instantiation
        # This is the instance that our custom MockStreamer will emulate
        self.mock_streamer_instance = MockStreamer(None, True, True) 
        # This is a mock for the TextIteratorStreamer CLASS itself.
        # When TextIteratorStreamer(...) is called in app.py, it will return self.mock_streamer_instance.
        self.mock_text_iterator_streamer_class = MagicMock(return_value=self.mock_streamer_instance)

        # Start patches
        # Patch the actual from_pretrained methods in the transformers library
        self.patch_tokenizer_from_pretrained = patch('transformers.AutoTokenizer.from_pretrained', return_value=self.mock_tokenizer_instance)
        self.patch_model_from_pretrained = patch('transformers.AutoModelForCausalLM.from_pretrained', return_value=self.mock_model_instance)
        
        # Patch TextIteratorStreamer where it's imported and used in app.py.
        # Assuming `from transformers import TextIteratorStreamer` in app.py,
        # then `TextIteratorStreamer(...)` is used.
        # So we patch 'app.TextIteratorStreamer' or 'transformers.TextIteratorStreamer'
        # 'app.TextIteratorStreamer' is more specific if it's directly imported under that name scope in app.py
        self.patch_text_iterator_streamer = patch('app.TextIteratorStreamer', self.mock_text_iterator_streamer_class)
        # If app.py does `import transformers` and then `transformers.TextIteratorStreamer`, target would be different.
        # Given app.py: `from transformers import ..., TextIteratorStreamer, ...`
        # Patching `app.TextIteratorStreamer` is correct.

        self.mock_auto_tokenizer_cls = self.patch_tokenizer_from_pretrained.start()
        self.mock_auto_model_cls = self.patch_model_from_pretrained.start()
        self.mock_tis_cls_in_app = self.patch_text_iterator_streamer.start()

        self.addCleanup(self.patch_tokenizer_from_pretrained.stop)
        self.addCleanup(self.patch_model_from_pretrained.stop)
        self.addCleanup(self.patch_text_iterator_streamer.stop)

        # Now, when create_app() is called, its internal load_model()
        # will call the patched from_pretrained methods, getting our mocks.
        self.app = create_app()
        self.app.config['TESTING'] = True
        self.client = self.app.test_client()

        # Verify that from_pretrained was called (i.e., load_model was run with patches active)
        self.mock_auto_tokenizer_cls.assert_called()
        self.mock_auto_model_cls.assert_called()


    def tearDown(self):
        # Patches are stopped by addCleanup
        pass

    # ... (test_index_page as in Fix 1) ...
    def test_index_page(self):
        response = self.client.get('/')
        self.assertEqual(response.status_code, 200)
        self.assertIn('聊天机器人', response.data.decode('utf-8'))


    def test_chat_stream(self):
        response = self.client.post('/chat', json={'message': '你好', 'history': []})
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.mimetype, 'text/event-stream')

        stream_data = b""
        # response.response is the iterator for streamed chunks in test client
        for chunk in response.response: 
            stream_data += chunk
        decoded_data = stream_data.decode('utf-8')
        
        # Verify TextIteratorStreamer class was instantiated correctly in app.py
        # self.mock_text_iterator_streamer_class (which is self.mock_tis_cls_in_app)
        self.mock_tis_cls_in_app.assert_called_once_with(self.mock_tokenizer_instance, skip_prompt=True, skip_special_tokens=True)
        
        # Verify tokenizer methods were called
        self.mock_tokenizer_instance.apply_chat_template.assert_called_once()
        args_template, kwargs_template = self.mock_tokenizer_instance.apply_chat_template.call_args
        self.assertEqual(args_template[0], [{"role": "user", "content": "你好"}])
        self.assertTrue(kwargs_template.get('add_generation_prompt'))

        # Verify tokenizer was called to prepare inputs: tokenizer([text], ...)
        # self.mock_tokenizer_instance is the mock for AutoTokenizer.from_pretrained
        # self.mock_tokenizer_instance.return_value is the mock for tokenizer(...) output
        self.mock_tokenizer_instance.assert_called_with(['Formatted prompt'], return_tensors="pt")
        # And then .to(device) was called on its result
        self.mock_tokenizer_instance.return_value.to.assert_called_once_with('cpu')
        
        # Verify model.generate was called.
        # Since it's in a thread, direct call count might be tricky if not synchronized.
        # However, if the streamer produced data, it implies generate was called.
        # A simple assert_called() might suffice if we are sure it's called at least once.
        self.mock_model_instance.generate.assert_called()
        # You can be more specific about arguments if needed, e.g., checking streamer instance
        # generate_args, generate_kwargs = self.mock_model_instance.generate.call_args
        # self.assertEqual(generate_kwargs['streamer'], self.mock_streamer_instance)


        expected_sse_data = "data: Hello\n\ndata:  \n\ndata: World\n\ndata: !\n\n"
        self.assertEqual(decoded_data, expected_sse_data)

    def test_chat_no_message(self):
        response = self.client.post('/chat', json={})
        self.assertEqual(response.status_code, 400)
        json_data = response.get_json()
        self.assertEqual(json_data['error'], 'No message provided')

    def test_chat_with_history(self):
        history = [{"role": "user", "content": "之前的问题"}, {"role": "assistant", "content": "之前的回答"}]
        response = self.client.post('/chat', json={'message': '新问题', 'history': history})
        self.assertEqual(response.status_code, 200)
        
        self.mock_tokenizer_instance.apply_chat_template.assert_called_once()
        args, kwargs = self.mock_tokenizer_instance.apply_chat_template.call_args
        expected_messages = [
            {"role": "user", "content": "之前的问题"},
            {"role": "assistant", "content": "之前的回答"},
            {"role": "user", "content": "新问题"}
        ]
        self.assertEqual(args[0], expected_messages)
        # Ensure other mocks are behaving as expected (e.g., generate called)
        self.mock_model_instance.generate.assert_called()

if __name__ == '__main__':
    unittest.main()
```

**解释修复3中的改动:**

1.  **`MockStreamer`**: 添加了 `put` 和 `end` 方法的空实现。
2.  **Patching `from_pretrained`**:
    *   `patch('transformers.AutoTokenizer.from_pretrained', return_value=self.mock_tokenizer_instance)`: 当 `app.py` 中的 `load_model` 调用 `AutoTokenizer.from_pretrained(...)` 时，它将直接获得 `self.mock_tokenizer_instance`，而不会执行真实的加载。
    *   同理 `patch('transformers.AutoModelForCausalLM.from_pretrained', ...)`。
3.  **Patching `TextIteratorStreamer`**:
    *   `patch('app.TextIteratorStreamer', self.mock_text_iterator_streamer_class)`: 当 `app.py` 中执行 `streamer = TextIteratorStreamer(...)` 时，实际上是调用了 `self.mock_text_iterator_streamer_class(...)`，这个 mock 类会返回我们预设的 `self.mock_streamer_instance` (即 `MockStreamer` 的实例)。
4.  **Tokenizer Call Mocking**:
    *   `self.mock_tokenizer_instance = MagicMock(spec=AutoTokenizer)`: 这是 `from_pretrained` 返回的 mock tokenizer。
    *   `self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"`: Mocking `apply_chat_template`.
    *   `mock_tokenized_output = MagicMock()`
    *   `mock_tokenized_output.to.return_value = mock_inputs_on_device`
    *   `self.mock_tokenizer_instance.return_value = mock_tokenized_output`: 这模拟了 `tokenizer(...)` 被调用时的行为。当 `app.py` 中写 `tokenizer([text], ...)`，它实际上是调用了 `self.mock_tokenizer_instance.__call__(...)` (因为 `tokenizer` 是个可调用对象)，这个调用会返回 `mock_tokenized_output`。然后 `mock_tokenized_output.to(...)` 会被调用。
5.  **Assertions in `setUp`**: `self.mock_auto_tokenizer_cls.assert_called()` 确保了 `AutoTokenizer.from_pretrained` (我们的patch目标) 确实在 `create_app()` 期间被调用了，证明 patch 生效并且 `load_model` 逻辑被执行（但使用的是 mock）。
6.  **Assertions in `test_chat_stream`**:
    *   `self.mock_tis_cls_in_app.assert_called_once_with(...)`: 确认 `TextIteratorStreamer` (我们的 mock 类) 被正确实例化。
    *   `self.mock_tokenizer_instance.apply_chat_template.assert_called_once()`: 确认模板应用被调用。
    *   `self.mock_tokenizer_instance.assert_called_with(...)`: 确认 `tokenizer(...)` (即 `__call__` 方法) 被以特定参数调用。
    *   `self.mock_tokenizer_instance.return_value.to.assert_called_once_with('cpu')`: 确认 `.to('cpu')` 在 `tokenizer(...)` 的结果上被调用。
    *   `self.mock_model_instance.generate.assert_called()`: 确认模型的 `generate` 方法被调用。由于它在线程中，`assert_called_once()` 可能因竞态条件失败，`assert_called()` 更宽松。如果流式数据能正确返回，也间接证明了 `generate` 的执行。

---

**问题4: `FAIL: test_index_page ... AssertionError: b'chatai' not found in ...`**

这其实与问题1相关，你当时测试的是 `b'chatai'`，而HTML标题是"聊天机器人"。修复1中已经改为检查"聊天机器人"。

**再次运行测试**

将 `tests/test_app.py` 文件更新为上述包含修复2和修复3的版本。然后再次运行：
`python -m unittest tests.test_app`

如果仍然有 "Sliding Window Attention..." 这类真实模型加载的日志，那说明 patch 可能有更深层次的问题，或者 `transformers` 库在导入时就执行了一些模型相关的初始化。但理论上，patch `from_pretrained` 应该是阻止真实模型对象创建的最有效方法。

请尝试这些修改，看看测试结果如何。
===== User =====
按照上述内容修改了单元测试代码后还是出现错误 应该怎么改进单元测试代码 错误如下
(chatbot_env) PS D:\file\桌面\软件测试技术\chatai> python -m unittest tests.test_app
EEEE
======================================================================
ERROR: test_chat_no_message (tests.test_app.ChatbotAppTestCase.test_chat_no_message)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 20, in setUp
    self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 660, in __getattr__  
    raise AttributeError("Mock object has no attribute %r" % name)
AttributeError: Mock object has no attribute 'apply_chat_template'

======================================================================
ERROR: test_chat_stream (tests.test_app.ChatbotAppTestCase.test_chat_stream)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 20, in setUp
    self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 660, in __getattr__  
    raise AttributeError("Mock object has no attribute %r" % name)
AttributeError: Mock object has no attribute 'apply_chat_template'

======================================================================
ERROR: test_chat_with_history (tests.test_app.ChatbotAppTestCase.test_chat_with_history)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 20, in setUp
    self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 660, in __getattr__  
    raise AttributeError("Mock object has no attribute %r" % name)
AttributeError: Mock object has no attribute 'apply_chat_template'

======================================================================
ERROR: test_index_page (tests.test_app.ChatbotAppTestCase.test_index_page)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\file\桌面\软件测试技术\chatai\tests\test_app.py", line 20, in setUp
    self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\install\Anaconda3\envs\chatbot_env\Lib\unittest\mock.py", line 660, in __getattr__  
    raise AttributeError("Mock object has no attribute %r" % name)
AttributeError: Mock object has no attribute 'apply_chat_template'

----------------------------------------------------------------------
Ran 4 tests in 0.010s

FAILED (errors=4)
===== AI =====
 啊，我明白了！这个错误 `AttributeError: Mock object has no attribute 'apply_chat_template'` 是因为我们在创建 `MagicMock` 时，如果使用了 `spec` 参数，那么这个 mock 对象就只能拥有 `spec` 指定的类所拥有的属性和方法。

`AutoTokenizer` 本身是一个工厂类，它的实例（即 `tokenizer` 对象）才拥有 `apply_chat_template` 方法。
而我们之前的代码是：
```python
self.mock_tokenizer_instance = MagicMock(spec=AutoTokenizer) # spec 是类本身
# ... 后来尝试给这个 mock_tokenizer_instance 设置 apply_chat_template
```
`AutoTokenizer` 类本身没有 `apply_chat_template` 方法，所以 `MagicMock(spec=AutoTokenizer)` 也没有。

**正确的做法是：**

1.  `AutoTokenizer.from_pretrained` 返回的是一个 tokenizer 实例。所以 `self.mock_tokenizer_instance` 应该是一个模拟这个 *实例* 的 mock。
2.  `spec` 参数应该指向那个 *实例* 的类型。但是，`AutoTokenizer` 的具体实例类型（如 `QWenTokenizer`）可能在 `transformers` 内部，我们不容易直接指定。
3.  一个更简单且通常有效的方法是，在创建 `MagicMock` 时，**不立即使用 `spec`**，或者如果确实需要 `spec` 来做类型检查，应该在创建 `MagicMock` 后，**手动添加你需要 mock 的方法**。
4.  或者，更精确地，`AutoTokenizer.from_pretrained` 是一个类方法。我们 patch 这个方法，让它返回一个已经配置好的 mock *实例*。

让我们调整 `setUp` 中的 mock 初始化部分。

**修复方案：**

我们将修改 `self.mock_tokenizer_instance` 的创建和配置方式。

```python
# tests/test_app.py
import unittest
import sys
import os
from unittest.mock import patch, MagicMock

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app import create_app
# 明确地从 transformers 导入被 mock 的类/函数
from transformers import AutoTokenizer, AutoModelForCausalLM # TextIteratorStreamer 已经在 app.py 中导入

# MockStreamer class definition (as before)
class MockStreamer:
    def __init__(self, tokenizer, skip_prompt, skip_special_tokens):
        self.tokenizer = tokenizer
        self.skip_prompt = skip_prompt
        self.skip_special_tokens = skip_special_tokens
    def put(self, value):
        pass
    def end(self):
        pass
    def __iter__(self):
        yield "Hello"
        yield " "
        yield "World"
        yield "!"
    # __next__ can be omitted if __iter__ is sufficient

class ChatbotAppTestCase(unittest.TestCase):

    def setUp(self):
        # 1. Mock for the tokenizer INSTANCE that AutoTokenizer.from_pretrained would return
        self.mock_tokenizer_instance = MagicMock() # Create a general MagicMock first
        # Now, configure the methods for this mock instance
        self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt" # This should now work
        
        mock_tokenized_output = MagicMock()
        mock_inputs_on_device = MagicMock(input_ids="mock_input_ids", attention_mask="mock_attention_mask")
        mock_tokenized_output.to.return_value = mock_inputs_on_device
        # Configure what tokenizer(...) (i.e., mock_tokenizer_instance.__call__) returns
        self.mock_tokenizer_instance.return_value = mock_tokenized_output # For tokenizer(...)

        # 2. Mock for the model INSTANCE that AutoModelForCausalLM.from_pretrained would return
        self.mock_model_instance = MagicMock() # General MagicMock
        # model.generate will be configured/asserted in tests

        # 3. Mock for TextIteratorStreamer instantiation
        self.mock_streamer_instance = MockStreamer(self.mock_tokenizer_instance, True, True) 
        self.mock_text_iterator_streamer_class = MagicMock(return_value=self.mock_streamer_instance)

        # Start patches
        # Patch AutoTokenizer.from_pretrained to return our pre-configured mock_tokenizer_instance
        self.patch_tokenizer_from_pretrained = patch('transformers.AutoTokenizer.from_pretrained', return_value=self.mock_tokenizer_instance)
        # Patch AutoModelForCausalLM.from_pretrained to return our pre-configured mock_model_instance
        self.patch_model_from_pretrained = patch('transformers.AutoModelForCausalLM.from_pretrained', return_value=self.mock_model_instance)
        
        # Patch TextIteratorStreamer class in app.py
        self.patch_text_iterator_streamer = patch('app.TextIteratorStreamer', self.mock_text_iterator_streamer_class)

        self.mock_auto_tokenizer_from_pretrained_method = self.patch_tokenizer_from_pretrained.start()
        self.mock_auto_model_from_pretrained_method = self.patch_model_from_pretrained.start()
        self.mock_tis_cls_in_app = self.patch_text_iterator_streamer.start()

        self.addCleanup(self.patch_tokenizer_from_pretrained.stop)
        self.addCleanup(self.patch_model_from_pretrained.stop)
        self.addCleanup(self.patch_text_iterator_streamer.stop)

        # Create the app AFTER patches are in place
        self.app = create_app()
        self.app.config['TESTING'] = True
        self.client = self.app.test_client()

        # Verify that from_pretrained methods were called (i.e., load_model was run with patches active)
        self.mock_auto_tokenizer_from_pretrained_method.assert_called_once()
        self.mock_auto_model_from_pretrained_method.assert_called_once()

    def tearDown(self):
        pass # Patches are stopped by addCleanup

    def test_index_page(self):
        response = self.client.get('/')
        self.assertEqual(response.status_code, 200)
        self.assertIn('聊天机器人', response.data.decode('utf-8'))

    def test_chat_stream(self):
        response = self.client.post('/chat', json={'message': '你好', 'history': []})
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.mimetype, 'text/event-stream')

        stream_data = b""
        for chunk in response.response: 
            stream_data += chunk
        decoded_data = stream_data.decode('utf-8')
        
        # Verify TextIteratorStreamer class was instantiated
        self.mock_tis_cls_in_app.assert_called_once_with(self.mock_tokenizer_instance, skip_prompt=True, skip_special_tokens=True)
        
        # Verify tokenizer instance methods were called
        self.mock_tokenizer_instance.apply_chat_template.assert_called_once()
        args_template, kwargs_template = self.mock_tokenizer_instance.apply_chat_template.call_args
        self.assertEqual(args_template[0], [{"role": "user", "content": "你好"}])
        self.assertTrue(kwargs_template.get('add_generation_prompt'))

        # Verify tokenizer instance was called (as a function)
        self.mock_tokenizer_instance.assert_called_with(['Formatted prompt'], return_tensors="pt")
        # Verify .to(device) was called on the result of tokenizer(...)
        # self.mock_tokenizer_instance.return_value is the mock for tokenizer(...) output
        self.mock_tokenizer_instance.return_value.to.assert_called_once_with('cpu')
        
        # Verify model.generate was called on the model instance
        self.mock_model_instance.generate.assert_called_once() 
        # More specific checks for generate arguments if needed:
        # _, generate_kwargs = self.mock_model_instance.generate.call_args
        # self.assertEqual(generate_kwargs['input_ids'], "mock_input_ids")
        # self.assertEqual(generate_kwargs['streamer'], self.mock_streamer_instance)

        expected_sse_data = "data: Hello\n\ndata:  \n\ndata: World\n\ndata: !\n\n"
        self.assertEqual(decoded_data, expected_sse_data)

    def test_chat_no_message(self):
        # This test doesn't involve the model/tokenizer mocks directly from setUp in its main flow,
        # but the app setup (create_app) still runs.
        response = self.client.post('/chat', json={})
        self.assertEqual(response.status_code, 400)
        json_data = response.get_json()
        self.assertEqual(json_data['error'], 'No message provided')

    def test_chat_with_history(self):
        history = [{"role": "user", "content": "之前的问题"}, {"role": "assistant", "content": "之前的回答"}]
        response = self.client.post('/chat', json={'message': '新问题', 'history': history})
        self.assertEqual(response.status_code, 200)
        
        self.mock_tokenizer_instance.apply_chat_template.assert_called_once()
        args, kwargs = self.mock_tokenizer_instance.apply_chat_template.call_args
        expected_messages = [
            {"role": "user", "content": "之前的问题"},
            {"role": "assistant", "content": "之前的回答"},
            {"role": "user", "content": "新问题"}
        ]
        self.assertEqual(args[0], expected_messages)
        self.mock_model_instance.generate.assert_called_once()

if __name__ == '__main__':
    unittest.main()
```

**关键改动解释：**

1.  **`self.mock_tokenizer_instance = MagicMock()`**: 我们创建了一个通用的 `MagicMock`。因为没有 `spec`，它可以拥有任何你赋值给它的属性或方法。
2.  **`self.mock_tokenizer_instance.apply_chat_template.return_value = "Formatted prompt"`**: 现在我们可以安全地为这个 `mock_tokenizer_instance` 添加 `apply_chat_template` 属性 (它本身也是一个 mock) 并设置其 `return_value`。
3.  **`self.mock_tokenizer_instance.return_value = mock_tokenized_output`**: 这里设置的是当 `self.mock_tokenizer_instance` *被调用时* (即 `tokenizer(...)`) 的返回值。`MagicMock` 对象默认是可调用的。
4.  **`self.mock_model_instance = MagicMock()`**: 类似地为模型实例创建一个通用的 mock。
5.  **Patching `from_pretrained`**: `patch('transformers.AutoTokenizer.from_pretrained', return_value=self.mock_tokenizer_instance)` 意味着当 `app.py` 中的 `AutoTokenizer.from_pretrained` 被调用时，它将直接返回我们已经完全配置好的 `self.mock_tokenizer_instance`。
6.  **Assertions**:
    *   `self.mock_auto_tokenizer_from_pretrained_method.assert_called_once()`: 确保 `AutoTokenizer.from_pretrained` 这个*方法*被调用了。
    *   在 `test_chat_stream` 和 `test_chat_with_history` 中，我们现在断言 `self.mock_tokenizer_instance.apply_chat_template` (模拟的实例方法) 和 `self.mock_model_instance.generate` (模拟的实例方法) 被调用。
    *   `self.mock_tokenizer_instance.assert_called_with(...)`: 验证 `tokenizer(...)` (即 `self.mock_tokenizer_instance` 作为函数被调用) 的参数。

这种方法不依赖于猜测 `AutoTokenizer.from_pretrained` 返回的具体实例类型，而是直接控制该方法返回的 mock 对象的行为。

请用这个更新后的 `tests/test_app.py` 再试一次。这应该能解决 `AttributeError`。
===== User =====
好的，现在代码已经能够重新运行，但是我发现一点瑕疵:问题生成过程中还能继续发送问题 这会导致两个问题回答冲突。 如果模型在生成回答时能够禁用发送功能按钮，让按钮显示加载中的情况，是不是能够避免回答冲突。
请修改对应代码并添加单元测试代码
===== AI =====
好的，你提出了一个很好的用户体验和健壮性改进点！在模型生成回复期间禁用发送按钮，并显示加载状态，可以有效避免用户连续快速提问导致的混乱。

我们将分两部分进行修改：

1.  **前端 (`static/script.js`)**: 修改 `sendMessage` 函数，在请求开始时禁用按钮并显示加载状态，在请求完成（无论成功或失败）后恢复按钮。
2.  **后端 (`app.py`)**: 后端代码基本不需要修改，因为这个行为主要由前端控制。
3.  **单元测试 (`tests/test_app.py`)**: 我们不需要为这个前端行为添加后端单元测试，因为后端API的行为没有改变。前端的这种UI交互逻辑通常通过端到端测试（如Selenium）或专门的前端单元/集成测试框架（如Jest, Vitest, Cypress）来测试。对于这个简单的HTML/JS，我们暂时不引入复杂的前端测试框架，但会确保逻辑正确。

**步骤 1: 修改前端 (`static/script.js`)**

我们将给发送按钮添加一个 "加载中..." 的文本或者一个简单的CSS动画。

```javascript
// static/script.js
document.addEventListener('DOMContentLoaded', () => {
    const chatBox = document.getElementById('chat-box');
    const userInput = document.getElementById('user-input');
    const sendButton = document.getElementById('send-button');
    const originalButtonText = sendButton.textContent; // 保存按钮原始文本
    let chatHistory = [];

    userInput.addEventListener('input', () => {
        userInput.style.height = 'auto';
        userInput.style.height = userInput.scrollHeight + 'px';
    });

    sendButton.addEventListener('click', sendMessage);
    userInput.addEventListener('keypress', (event) => {
        if (event.key === 'Enter' && !event.shiftKey) {
            event.preventDefault();
            sendMessage();
        }
    });

    function appendMessage(text, sender) {
        const messageDiv = document.createElement('div');
        messageDiv.classList.add('message', sender === 'user' ? 'user-message' : 'bot-message');
        
        const textNode = document.createTextNode(text);
        messageDiv.appendChild(textNode);
        
        chatBox.appendChild(messageDiv);
        chatBox.scrollTop = chatBox.scrollHeight;
        return messageDiv;
    }

    function setButtonLoading(isLoading) {
        if (isLoading) {
            sendButton.disabled = true;
            sendButton.textContent = '生成中...'; // 或者使用 spinner 图标
            // 可以添加一个 CSS class 来改变样式，例如:
            // sendButton.classList.add('loading');
        } else {
            sendButton.disabled = false;
            sendButton.textContent = originalButtonText;
            // sendButton.classList.remove('loading');
        }
    }

    async function sendMessage() {
        const messageText = userInput.value.trim();
        if (messageText === '' || sendButton.disabled) return; // 如果按钮已禁用，则不执行

        appendMessage(messageText, 'user');
        chatHistory.push({ role: "user", content: messageText });
        userInput.value = '';
        userInput.style.height = 'auto';
        
        setButtonLoading(true); // <--- 禁用按钮并显示加载状态

        let botMessageDiv = appendMessage('', 'bot');
        botMessageDiv.innerHTML = '<div class="typing-indicator"><span></span><span></span><span></span></div>';

        try {
            const response = await fetch('/chat', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    message: messageText,
                    history: chatHistory
                }),
            });

            if (!response.ok) {
                let errorText = `服务器错误: ${response.status}`;
                try {
                    const errorData = await response.json();
                    errorText = errorData.error || errorText;
                } catch (e) {
                    errorText = await response.text() || errorText;
                }
                console.error('Error from server:', errorText);
                botMessageDiv.textContent = `错误: ${errorText}`;
                // chatHistory.pop(); // 考虑是否移除，取决于错误类型
                return; // 在 finally 中恢复按钮
            }
            
            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let fullBotResponse = "";
            botMessageDiv.innerHTML = ""; 

            while (true) {
                const { value, done } = await reader.read();
                if (done) {
                    break;
                }
                const chunk = decoder.decode(value, { stream: true });
                const lines = chunk.split('\n');
                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.substring(6).trim();
                        if (data === "[END_OF_STREAM]") {
                            // stream ended
                        } else if (data.startsWith("[ERROR]")) {
                            botMessageDiv.textContent += data.substring(7).trim();
                            fullBotResponse += data.substring(7).trim();
                        } else {
                            botMessageDiv.textContent += data;
                            fullBotResponse += data;
                            chatBox.scrollTop = chatBox.scrollHeight;
                        }
                    }
                }
            }
            if (fullBotResponse) {
                chatHistory.push({ role: "assistant", content: fullBotResponse });
            } else {
                chatBox.removeChild(botMessageDiv);
            }

        } catch (error) {
            console.error('Error sending message:', error);
            botMessageDiv.textContent = '发生网络错误，请稍后再试。';
        } finally {
            setButtonLoading(false); // <--- 在 finally 块中恢复按钮状态
        }
    }
});
```

**前端修改解释:**

1.  **`originalButtonText`**: 在 `DOMContentLoaded` 时保存按钮的原始文本（例如 "发送"）。
2.  **`setButtonLoading(isLoading)` 函数**:
    *   封装了设置按钮加载状态的逻辑。
    *   `isLoading = true`: 禁用按钮，并将文本更改为 "生成中..."。
    *   `isLoading = false`: 启用按钮，并恢复其原始文本。
    *   注释中提到了可以添加/移除 CSS 类来实现更复杂的加载动画（例如，一个旋转的 spinner）。
3.  **`sendMessage` 函数修改**:
    *   在 `try` 块之前调用 `setButtonLoading(true)`。
    *   使用 `finally` 块来确保无论请求成功、失败还是发生 JavaScript 错误，`setButtonLoading(false)` 都会被调用，从而恢复按钮状态。
    *   在 `sendMessage` 开头添加 `if (sendButton.disabled) return;` 进一步防止在按钮禁用时意外触发发送。

**可选：为加载状态添加 CSS 样式 (在 `static/style.css`)**

如果你想用 CSS 类来控制加载状态，可以这样做：

```css
/* static/style.css */
/* ... 其他样式 ... */

#send-button.loading {
    background-color: #0056b3; /* 一个稍暗的颜色表示加载 */
    cursor: wait; /* 更改鼠标指针 */
}

/* 如果你想用 spinner，可以添加如下样式 (需要 HTML 中有对应元素或 JS 动态添加) */
/*
#send-button.loading::after {
    content: '';
    display: inline-block;
    width: 1em;
    height: 1em;
    margin-left: 8px;
    border: 2px solid #fff;
    border-top-color: transparent;
    border-radius: 50%;
    animation: spin 0.6s linear infinite;
}

@keyframes spin {
    to { transform: rotate(360deg); }
}
*/
```
如果使用 CSS 类，`setButtonLoading` 函数需要相应修改：
```javascript
// In script.js
// function setButtonLoading(isLoading) {
//     if (isLoading) {
//         sendButton.disabled = true;
//         sendButton.textContent = '生成中...'; // 或者只改变 class，让 CSS 处理文本/图标
//         sendButton.classList.add('loading');
//     } else {
//         sendButton.disabled = false;
//         sendButton.textContent = originalButtonText;
//         sendButton.classList.remove('loading');
//     }
// }
```
选择简单的文本改变还是 CSS 类+动画取决于你的偏好。文本改变是最简单直接的。

**步骤 2: 后端 (`app.py`)**

后端代码不需要为此功能进行修改。流式传输和生成逻辑保持不变。

**步骤 3: 关于单元测试**

如前所述，这种纯前端的 UI 状态改变（禁用按钮、改变文本）通常不通过后端的 Python `unittest` 来测试。后端单元测试关注的是 API 的输入、输出和内部逻辑。

*   **为什么后端单元测试不适合？**
    *   后端测试无法直接操作或断言前端 DOM 元素的状态。
    *   后端测试并不知道前端按钮的存在或其状态。

*   **如何测试这种前端行为？**
    1.  **手动测试**: 最基本的方法，在浏览器中操作并观察。
    2.  **前端单元/集成测试**: 使用像 Jest、Vitest 配合 testing-library/react (如果用React) 或 testing-library/dom (用于原生JS/HTML) 来编写测试。这些测试会在模拟的浏览器环境 (如 JSDOM) 中运行，可以检查按钮的 `disabled` 属性和 `textContent`。
        例如 (伪代码，使用类 Jest 的语法):
        ```javascript
        // __tests__/chat.test.js
        describe('Chat UI', () => {
            it('should disable send button during message generation and re-enable it after', async () => {
                // Setup: mock fetch, initialize DOM elements
                document.body.innerHTML = `...your HTML structure...`;
                const sendButton = document.getElementById('send-button');
                const userInput = document.getElementById('user-input');
                
                // Mock fetch to simulate a delay and successful response
                global.fetch = jest.fn(() =>
                    Promise.resolve({
                        ok: true,
                        body: {
                            getReader: () => ({
                                read: jest.fn()
                                    .mockResolvedValueOnce({ value: new TextEncoder().encode("data: Hello\n\n"), done: false })
                                    .mockResolvedValueOnce({ value: new TextEncoder().encode("data: World\n\n"), done: false })
                                    .mockResolvedValueOnce({ value: new Uint8Array(), done: true }),
                            }),
                        },
                    })
                );

                // Action: type message and click send
                userInput.value = "Test message";
                sendButton.click(); // This will trigger sendMessage

                // Assertion: button should be disabled immediately (or very soon)
                expect(sendButton.disabled).toBe(true);
                expect(sendButton.textContent).toBe('生成中...');

                // Wait for the fetch mock and stream processing to complete
                // (This might require more sophisticated waiting mechanisms like `waitFor` from testing-library)
                await new Promise(resolve => setTimeout(resolve, 100)); // Simple timeout, adjust as needed

                // Assertion: button should be re-enabled
                expect(sendButton.disabled).toBe(false);
                expect(sendButton.textContent).toBe('发送'); // Assuming '发送' is originalButtonText
            });
        });
        ```
    3.  **端到端测试 (E2E)**: 使用 Selenium, Playwright, 或 Cypress。这些工具会启动一个真实的浏览器，模拟用户操作，并可以检查页面元素的实际状态。这是最全面的测试，但也最慢、最脆弱。

**总结**

我们已经通过修改前端 JavaScript 代码实现了在模型生成回复期间禁用发送按钮并显示加载状态的功能。这显著改善了用户体验，防止了消息冲突。对于这种 UI 行为的自动化测试，推荐使用前端测试工具。

现在，当你运行应用并发送消息时，发送按钮应该会变为 "生成中..." 并且不可点击，直到回复完全显示后恢复原状。

===== User =====

===== AI =====
 
===== User =====

===== AI =====
 
===== User =====

===== AI =====
 
===== User =====

===== AI =====
 
===== User =====

===== AI =====
 